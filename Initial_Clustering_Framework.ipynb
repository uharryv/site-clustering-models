{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this code is to identify the optimal commute division factor for a given proximity constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m geodesic\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DBSCAN, KMeans\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### only change values in this cell\n",
    "\n",
    "DEFAULT_COMMUTE_RADIUS = 150 \n",
    "MIN_SITES = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_locations(file_path):\n",
    "    \"\"\"Load location data from Excel file\"\"\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def calculate_distance_miles(coord1, coord2):\n",
    "    \"\"\"Calculate distance between two coordinates in miles\"\"\"\n",
    "    return geodesic(coord1, coord2).miles\n",
    "\n",
    "def safe_join(series):\n",
    "    \"\"\"Join series values handling different data types and null values\"\"\"\n",
    "    if series is None or len(series) == 0:\n",
    "        return \"\"\n",
    "    return ', '.join(str(x) for x in pd.Series(series).dropna().unique() if str(x).strip() != '')\n",
    "\n",
    "\n",
    "# Campus Size Category based on quartiles of OPS_H sum\n",
    "def get_size_category(ops_h_sum, q1, q2, q3):\n",
    "    \"\"\"Classify campus size based on OPS_H sum quartiles\"\"\"\n",
    "    if ops_h_sum < q1:\n",
    "        return \"Small\"\n",
    "    elif ops_h_sum < q2:\n",
    "        return \"Medium\"\n",
    "    elif ops_h_sum < q3:\n",
    "        return \"Large\"\n",
    "    else:\n",
    "        return \"X-Large\"\n",
    "\n",
    "\n",
    "# HR Staffing Levels (base rules)\n",
    "def get_hr_staffing_levels(size_category):\n",
    "    \"\"\"\n",
    "    Returns recommended HR staffing levels based on size category.\n",
    "    Base staffing:\n",
    "      X-Large: Campus Leader=1, HR_L4=16, HR_L5=6, HR_L6=5, HR_L7=2\n",
    "      Large:   Campus Leader=1, HR_L4=11, HR_L5=5, HR_L6=3, HR_L7=1\n",
    "      Medium:  Campus Leader=1, HR_L4=8,  HR_L5=4, HR_L6=2, HR_L7=0\n",
    "      Small:   Campus Leader=1, HR_L4=5,  HR_L5=3, HR_L6=2, HR_L7=0\n",
    "      Multi-Site: Campus Leader=1, HR_L4=1, HR_L5=1, HR_L6=0, HR_L7=0\n",
    "    \"\"\"\n",
    "    staffing_matrix = {\n",
    "        'Small': {\n",
    "            'Campus_Leader': 1,\n",
    "            'HR_L4': 5,\n",
    "            'HR_L5': 3,\n",
    "            'HR_L6': 2,\n",
    "            'HR_L7': 0\n",
    "        },\n",
    "        'Medium': {\n",
    "            'Campus_Leader': 1,\n",
    "            'HR_L4': 8,\n",
    "            'HR_L5': 4,\n",
    "            'HR_L6': 2,\n",
    "            'HR_L7': 0\n",
    "        },\n",
    "        'Large': {\n",
    "            'Campus_Leader': 1,\n",
    "            'HR_L4': 11,\n",
    "            'HR_L5': 5,\n",
    "            'HR_L6': 3,\n",
    "            'HR_L7': 1\n",
    "        },\n",
    "        'X-Large': {\n",
    "            'Campus_Leader': 1,\n",
    "            'HR_L4': 16,\n",
    "            'HR_L5': 6,\n",
    "            'HR_L6': 5,\n",
    "            'HR_L7': 2\n",
    "        },\n",
    "        'Multi-Site': {\n",
    "            'Campus_Leader': 1,\n",
    "            'HR_L4': 1,\n",
    "            'HR_L5': 1,\n",
    "            'HR_L6': 0,\n",
    "            'HR_L7': 0\n",
    "        }\n",
    "    }\n",
    "    return staffing_matrix.get(size_category, {\n",
    "        'Campus_Leader': 0,\n",
    "        'HR_L4': 0,\n",
    "        'HR_L5': 0,\n",
    "        'HR_L6': 0,\n",
    "        'HR_L7': 0\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Analyze clusters and perform HR/staffing analysis.\n",
    "def analyze_clusters(df):\n",
    "    \"\"\"Generate summary statistics and staffing analysis for each campus.\"\"\"\n",
    "    cluster_stats = []\n",
    "    cluster_ids = df['cluster_id'].unique()\n",
    "    \n",
    "    # Compute quartile thresholds for OPS_H from valid (non-outlier) clusters.\n",
    "    campus_sizes = []\n",
    "    for cid in cluster_ids:\n",
    "        cluster_data = df[df['cluster_id'] == cid]\n",
    "        if not cluster_data['is_outlier'].all():\n",
    "            campus_sizes.append(cluster_data['OPS_H'].sum())\n",
    "    if campus_sizes:\n",
    "        q1_size = np.percentile(campus_sizes, 25)\n",
    "        q2_size = np.percentile(campus_sizes, 50)\n",
    "        q3_size = np.percentile(campus_sizes, 75)\n",
    "    else:\n",
    "        q1_size, q2_size, q3_size = 15000, 30000, 50000\n",
    "\n",
    "    for cid in cluster_ids:\n",
    "        cluster_data = df[df['cluster_id'] == cid]\n",
    "        center_lat = cluster_data['latitude'].mean()\n",
    "        center_lon = cluster_data['longitude'].mean()\n",
    "        \n",
    "        max_distance = 0\n",
    "        sites = cluster_data[['latitude', 'longitude']].values\n",
    "        if len(sites) > 1:\n",
    "            for i in range(len(sites)):\n",
    "                for j in range(i+1, len(sites)):\n",
    "                    d = calculate_distance_miles(sites[i], sites[j])\n",
    "                    if d > max_distance:\n",
    "                        max_distance = d\n",
    "        \n",
    "        cluster_aa_hc = cluster_data['OPS_H'].sum()\n",
    "        cluster_ops_hc = cluster_data['OPS_S'].sum()\n",
    "        \n",
    "        # Current staffing summary (if such columns exist)\n",
    "        current_staffing = {\n",
    "            'Current_Campus_Leader': 1 if 'Campus_Leader' in cluster_data.columns else 0,\n",
    "            'Current_HR_L4': cluster_data['HR_4'].sum() if 'HR_4' in cluster_data.columns else 0,\n",
    "            'Current_HR_L5': cluster_data['HR_5'].sum() if 'HR_5' in cluster_data.columns else 0,\n",
    "            'Current_HR_L6': cluster_data['HR_6'].sum() if 'HR_6' in cluster_data.columns else 0,\n",
    "            'Current_HR_L7': cluster_data['HR_7'].sum() if 'HR_7' in cluster_data.columns else 0\n",
    "        }\n",
    "        current_hr_total = sum(current_staffing.values())\n",
    "        current_gearing_ratio = (cluster_aa_hc / current_hr_total) if current_hr_total else None\n",
    "        current_gearing_ratio_ops = (cluster_ops_hc / current_hr_total) if current_hr_total else None\n",
    "        \n",
    "        if cluster_data['multi_site'].any():\n",
    "            size_category = \"Multi-Site\"\n",
    "            new_hr_staffing = get_hr_staffing_levels(\"Multi-Site\")\n",
    "        else:\n",
    "            size_category = get_size_category(cluster_aa_hc, q1_size, q2_size, q3_size)\n",
    "            new_hr_staffing = get_hr_staffing_levels(size_category)\n",
    "        \n",
    "        new_hr_total = sum(new_hr_staffing.values())\n",
    "        new_gearing_ratio = (cluster_aa_hc / new_hr_total) if new_hr_total else None\n",
    "        new_gearing_ratio_ops = (cluster_ops_hc / new_hr_total) if new_hr_total else None\n",
    "        \n",
    "        stats = {\n",
    "            'cluster_id': cid,\n",
    "            'num_sites': len(cluster_data),\n",
    "            'size_category': size_category,\n",
    "            'Cluster_AA_HC': cluster_aa_hc,\n",
    "            'Cluster_OPS_HC': cluster_ops_hc,\n",
    "            'Current_Campus_Leader': current_staffing['Current_Campus_Leader'],\n",
    "            'Current_HR_L4': current_staffing['Current_HR_L4'],\n",
    "            'Current_HR_L5': current_staffing['Current_HR_L5'],\n",
    "            'Current_HR_L6': current_staffing['Current_HR_L6'],\n",
    "            'Current_HR_L7': current_staffing['Current_HR_L7'],\n",
    "            'Current_HR_Total': current_hr_total,\n",
    "            'Current_Gearing_Ratio_AA': current_gearing_ratio,\n",
    "            'Current_Gearing_Ratio_OPS': current_gearing_ratio_ops,\n",
    "            'New_Campus_Leader': new_hr_staffing['Campus_Leader'],\n",
    "            'New_HR_L4': new_hr_staffing['HR_L4'],\n",
    "            'New_HR_L5': new_hr_staffing['HR_L5'],\n",
    "            'New_HR_L6': new_hr_staffing['HR_L6'],\n",
    "            'New_HR_L7': new_hr_staffing['HR_L7'],\n",
    "            'New_HR_Total': new_hr_total,\n",
    "            'New_Gearing_Ratio_AA': new_gearing_ratio,\n",
    "            'New_Gearing_Ratio_OPS': new_gearing_ratio_ops,\n",
    "            'country': safe_join(cluster_data['country']),\n",
    "            'markets': safe_join(cluster_data['market']),\n",
    "            'cities': safe_join(cluster_data['city']),\n",
    "            'states': safe_join(cluster_data['state']),\n",
    "            'center_latitude': center_lat,\n",
    "            'center_longitude': center_lon,\n",
    "            'max_distance_miles': round(max_distance, 2),\n",
    "            'num_reassigned_outliers': len(cluster_data[cluster_data['reassignment_type'].notnull()])\n",
    "        }\n",
    "        cluster_stats.append(stats)\n",
    "    \n",
    "    df_stats = pd.DataFrame(cluster_stats)\n",
    "    # Round gearing ratios for presentation.\n",
    "    for col in ['Current_Gearing_Ratio_AA', 'Current_Gearing_Ratio_OPS',\n",
    "                'New_Gearing_Ratio_AA', 'New_Gearing_Ratio_OPS']:\n",
    "        df_stats[col] = df_stats[col].apply(lambda x: round(x, 2) if pd.notnull(x) else x)\n",
    "    \n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "##change path for input file\n",
    "df = pd.read_excel('locations.xlsx')\n",
    "\n",
    "## distance matrix b/w site locations\n",
    "\n",
    "coords = df[['latitude', 'longitude']].values\n",
    "dist_matrix = squareform(pdist(coords, metric=lambda u, v: calculate_distance_miles(u, v)))\n",
    "\n",
    "### clustering 1st pass\n",
    "df_clustered = df.copy()\n",
    "\n",
    "min_sites = MIN_SITES\n",
    "\n",
    "commute_division_options = [1,2,3,4,5,6,7,8,9,10]\n",
    "i=0\n",
    "\n",
    "num_clusters_list = []\n",
    "num_outliers_list = []\n",
    "perct_outliers_list = []\n",
    "num_clusters_above_commute_list = []\n",
    "perct_clusters_above_commute_list = []\n",
    "\n",
    "while i<len(commute_division_options):\n",
    "    print(i)\n",
    "    commute_radius = DEFAULT_COMMUTE_RADIUS/commute_division_options[i]\n",
    "    clustering = DBSCAN(eps=commute_radius, min_samples=min_sites, metric='precomputed')\n",
    "    cluster_labels = clustering.fit_predict(dist_matrix)\n",
    "    df_clustered['cluster_id'] = cluster_labels\n",
    "    df_clustered['initial_outlier'] = (cluster_labels == -1)\n",
    "    df_clustered['outlier_reassigned'] = False\n",
    "    df_clustered['is_outlier'] = (cluster_labels == -1)\n",
    "    df_clustered['multi_site'] = False\n",
    "    df_clustered['reassignment_type'] = None\n",
    "\n",
    "    #number of clusters\n",
    "    num_clusters_list.append(df_clustered['cluster_id'].nunique())\n",
    "\n",
    "    ### Analyse initial cluster\n",
    "    cluster_analysis = analyze_clusters(df_clustered)\n",
    "\n",
    "    ### #outliers grouped into -1 cluster\n",
    "    num_outliers_list.append(cluster_analysis[(cluster_analysis['cluster_id'] == -1)]['num_sites'].sum())\n",
    "    perct_outliers_list.append(cluster_analysis[(cluster_analysis['cluster_id'] == -1)]['num_sites'].sum() / cluster_analysis['num_sites'].sum())\n",
    "\n",
    "    ## except outliers, summary of remaining clusters\n",
    "    cluster_analysis_no_outlier = cluster_analysis[~(cluster_analysis['cluster_id'] == -1)]\n",
    "\n",
    "    ##how many clusters have max_distance > DEFAULT_COMMUTE_RADIUS\n",
    "    num_clusters_above_commute_list.append(len(cluster_analysis_no_outlier[cluster_analysis_no_outlier['max_distance_miles'] > DEFAULT_COMMUTE_RADIUS]))\n",
    "    perct_clusters_above_commute_list.append(len(cluster_analysis_no_outlier[cluster_analysis_no_outlier['max_distance_miles'] > DEFAULT_COMMUTE_RADIUS])/ len(cluster_analysis_no_outlier))\n",
    "\n",
    "    ##storing cluster data\n",
    "    #cluster_data.append(dict(zip(num_clusters,num_outliers.tolist(),perct_outliers.tolist(),num_clusters_above_commute,perct_clusters_above_commute)))\n",
    "    #cluster_summary = cluster_analysis_no_outlier[['num_sites','Cluster_AA_HC','max_distance_miles']].describe()\n",
    "\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commute_division_options</th>\n",
       "      <th>num_clusters</th>\n",
       "      <th>num_outliers</th>\n",
       "      <th>perct_outliers</th>\n",
       "      <th>num_clusters_with_max_dist_above_commute</th>\n",
       "      <th>perct_clusters_with_max_dist_above_commute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>109</td>\n",
       "      <td>0.097496</td>\n",
       "      <td>11</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>215</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>5</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>251</td>\n",
       "      <td>0.224508</td>\n",
       "      <td>3</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>274</td>\n",
       "      <td>0.245081</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>301</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>321</td>\n",
       "      <td>0.287120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>350</td>\n",
       "      <td>0.313059</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>397</td>\n",
       "      <td>0.355098</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>458</td>\n",
       "      <td>0.409660</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   commute_division_options  num_clusters  num_outliers  perct_outliers  \\\n",
       "0                         1             9            21        0.018784   \n",
       "1                         2            31           109        0.097496   \n",
       "2                         3            36           215        0.192308   \n",
       "3                         4            38           251        0.224508   \n",
       "4                         5            40           274        0.245081   \n",
       "5                         6            43           301        0.269231   \n",
       "6                         7            46           321        0.287120   \n",
       "7                         8            51           350        0.313059   \n",
       "8                         9            52           397        0.355098   \n",
       "9                        10            51           458        0.409660   \n",
       "\n",
       "   num_clusters_with_max_dist_above_commute  \\\n",
       "0                                         7   \n",
       "1                                        11   \n",
       "2                                         5   \n",
       "3                                         3   \n",
       "4                                         2   \n",
       "5                                         1   \n",
       "6                                         1   \n",
       "7                                         1   \n",
       "8                                         1   \n",
       "9                                         1   \n",
       "\n",
       "   perct_clusters_with_max_dist_above_commute  \n",
       "0                                    0.875000  \n",
       "1                                    0.366667  \n",
       "2                                    0.142857  \n",
       "3                                    0.081081  \n",
       "4                                    0.051282  \n",
       "5                                    0.023810  \n",
       "6                                    0.022222  \n",
       "7                                    0.020000  \n",
       "8                                    0.019608  \n",
       "9                                    0.020000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(list(zip(\n",
    "                commute_division_options,\n",
    "                num_clusters_list,\n",
    "                num_outliers_list,\n",
    "                perct_outliers_list,\n",
    "                num_clusters_above_commute_list,\n",
    "                perct_clusters_above_commute_list,\n",
    "            )),\n",
    "        columns=[\"commute_division_options\",\"num_clusters\",\"num_outliers\",\"perct_outliers\",\"num_clusters_with_max_dist_above_commute\",\"perct_clusters_with_max_dist_above_commute\",])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9813/2923228575.py:12: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  print(int(best_option['commute_division_options']))\n"
     ]
    }
   ],
   "source": [
    "### best commute division option\n",
    "\n",
    "min_clusters_with_max_dist_above_commute = int(report['num_clusters_with_max_dist_above_commute'].min())\n",
    "\n",
    "best_option = report[report['num_clusters_with_max_dist_above_commute']==min_clusters_with_max_dist_above_commute]\n",
    "\n",
    "min_outliers = int(best_option['num_outliers'].min())\n",
    "\n",
    "best_option = report[report['num_outliers']==min_outliers]\n",
    "\n",
    "### best commute divison option for given scenario\n",
    "print(int(best_option['commute_division_options']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
